# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V3Wl426ZyTW8M4kDUIqDlfmuwX2do3Q_
"""

from textblob import TextBlob
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter
from nltk.corpus import stopwords
import string
import numpy as np

import matplotlib.pyplot as plt
import pandas as pd

from googletrans import Translator

# Initialize the translator
translator = Translator()
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')


df = pd.read_csv('data/with classification.csv')
print(df['transcriptConsumer'])

"""***Translations***"""
# Define a function to translate sentences
def translate_sentence(sentence):
    print('sentence: ', sentence)
    if pd.notna(sentence):
        # Split the sentence into multiple sentences based on the newline character \n
        sentences = sentence.split("\\n")

        # Translate each part separately
        translated_sentences = [translator.translate(s, src='he', dest='en').text for s in sentences]
        print('translated_sentences: ', translated_sentences)
        # Join the translated sentences back together
        translated_sentence = "\n".join(translated_sentences)

        return translated_sentence
    else:
        return np.nan

# Initialize the Google Translate client
df['transcriptConsumerEnglish'] = translate_sentence(df['transcriptConsumer'][4])#df['transcriptConsumer'][0].apply(translate_sentence)
print(df[['transcriptConsumer', 'transcriptConsumerEnglish']])

"""***Tokenization***

1.   CC: Coordinating conjunction
2. CD: Cardinal number
3. DT: Determiner
4. EX: Existential there
5. FW: Foreign word
6. IN: Preposition or subordinating conjunction
7. JJ: Adjective
8. JJR: Adjective, comparative
9. JJS: Adjective, superlative
10. LS: List item marker
11. MD: Modal
12. NN: Noun, singular or mass
13. NNS: Noun, plural
14. NNP: Proper noun, singular
15. NNPS: Proper noun, plural
16. PDT: Predeterminer
17. POS: Possessive ending
18. PRP: Personal pronoun
19. PRP$: Possessive pronoun
20. RB: Adverb
21. RBR: Adverb, comparative
22. RBS: Adverb, superlative
23. RP: Particle
24. SYM: Symbol
25. TO: to
26. UH: Interjection
27. VB: Verb, base form
28. VBD: Verb, past tense
29. VBG: Verb, gerund or present participle
30. VBN: Verb, past participle
31. VBP: Verb, non-3rd person singular present
32. VBZ: Verb, 3rd person singular present
33. WDT: Wh-determiner
34. WP: Wh-pronoun
35. WP$: Possessive wh-pronoun
36. WRB: Wh-adverb
"""

tokens_english = nltk.word_tokenize(sentence_english)
print(tokens_english)

tagged_tokens_english = nltk.pos_tag(tokens_english)
tagged_tokens_english

# Count the appearance of each word
word_counts = Counter(tokens_english)

# Print the word counts
for word, count in word_counts.items():
    print(word, ':', count)

# Remove stopwords and punctuation
stop_words = set(stopwords.words('english'))
punctuation = set(string.punctuation)
stop_words

# Convert all words to lowercase
words_english = [word.lower() for word in tokens_english]

filtered_words = [word for word in words_english if word not in stop_words and word not in punctuation]
print("Filtered words:", filtered_words)

# Count the appearance of each word
word_counts = Counter(filtered_words)

# Plot the histogram of token frequencies
plt.figure(figsize=(12, 6))
plt.bar(word_counts.keys(), word_counts.values())
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Token Frequency Histogram')
plt.xticks(rotation=45)
plt.show()